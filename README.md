ğŸ“Š Data Processing Tool
Overview
Data Processing Tool is a metadata-driven ETL pipeline built in Python with SQL Server as the backend. It automates ingestion of source files (CSV/JSON) into a structured data warehouse organized into three layers:

Raw Layer â†’ Stores raw ingested data
Curated Layer â†’ Cleansed and standardized data
Processed Layer â†’ Final analytical tables with Slowly Changing Dimensions (SCD1/SCD2)

The pipeline supports full and incremental loads, dynamic DDL creation, audit logging, and includes a Streamlit UI for orchestration.

âœ¨ Features
Metadata-driven schema creation
Full & incremental load support
SCD1 and SCD2 handling
Audit trail with external logging
Streamlit UI for job execution and monitoring


ğŸ“‚ Project Structure
ETL-Job-Runner/
â”‚
â”œâ”€â”€ input_files/
â”‚   â”œâ”€â”€ config/        # Config Excel files
â”‚   â”œâ”€â”€ metadata/      # Metadata Excel files
â”‚   â””â”€â”€ data/          # Source data files
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ audit.py               # Job audit logging
â”‚   â”œâ”€â”€ create_ddl.py          # Dynamic DDL creation
â”‚   â”œâ”€â”€ curated_processed.py   # Curated â†’ Processed ETL
â”‚   â”œâ”€â”€ db_connection.py       # SQLAlchemy engine setup
â”‚   â”œâ”€â”€ load_config_metadata.py# Load Config & Metadata
â”‚   â”œâ”€â”€ load_type.py           # Full & Incremental load logic
â”‚   â”œâ”€â”€ main.py                # CLI entry point
â”‚   â”œâ”€â”€ orchestration.py       # Job orchestration
â”‚   â”œâ”€â”€ raw_curated.py         # Raw â†’ Curated ETL
â”‚   â”œâ”€â”€ scd_type.py            # SCD1 & SCD2 merge logic
â”‚   â”œâ”€â”€ send_log.py            # External logging
â”‚   â”œâ”€â”€ source_raw.py          # Source â†’ Raw ETL
â”‚   â””â”€â”€ validate_input.py      # Config & Metadata validation
â”‚
â”œâ”€â”€ ui_run.py                  # Streamlit UI
â”œâ”€â”€ ETLJobRunner.sql           # Database schema & control tables
â””â”€â”€ README.md


ğŸ—„ï¸ Database Setup
Run ETLJobRunner.sql in SQL Server to create:

Schemas: raw, curated, processed
Control tables: Config, Metadata, JobAudit, IncrementalTracker


âš™ï¸ How It Works
Config & Metadata Upload

Config defines job settings (source type, path, schema, load type, SCD type).
Metadata defines column mappings, data types, PK/FK constraints.

ETL Flow

Source â†’ Raw: Reads CSV/JSON, validates columns, inserts into raw tables.
Raw â†’ Curated: Cleans data, enforces PK/FK, casts types.
Curated â†’ Processed: Applies SCD logic (SCD1 overwrite, SCD2 history).

Audit & Logging

Logs job start/end with status and row counts.
Sends structured logs to external endpoint.


â–¶ï¸ Run Options
Command Line
python scripts/main.py --job JOB_EMP_RAW --config path/to/config.xlsx --metadata 
path/to/metadata.xlsx

Streamlit UI
streamlit run ui_run.py

ğŸ› ï¸ Tech Stack
Python: Pandas, SQLAlchemy, Streamlit
SQL Server: Storage & MERGE operations